---
title: 'Assignment1_DL_mpadhye'
output:
  html_document:
    df_print: paged
  html_notebook:
    highlight: textmate
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```


Two-class classification, or binary classification, may be the most widely applied kind of machine learning problem. In this example, we will learn to classify movie reviews into "positive" reviews and "negative" reviews, just based on the text content of the reviews.

## Load IMDB dataset

```{r, results='hide'}
library(keras)
library(dplyr)

imdb <- dataset_imdb(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb
```

The argument `num_words = 10000` means that we will only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. This allows us to work with vector data of manageable size.

The variables `train_data` and `test_data` are lists of reviews, each review being a list of word indices (encoding a sequence of words). `train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for "negative" and 1 stands for "positive":

```{r}
str(train_data[[1]])
```

```{r}
train_labels[[1]]
```

Since we restricted ourselves to the top 10,000 most frequent words, no word index will exceed 10,000:

```{r}
max(sapply(train_data, max))
```

## Preparing the data

```{r}
vectorize_sequences <- function(sequences, dimension = 10000) {
  # Create an all-zero matrix of shape (len(sequences), dimension)
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences))
    # Sets specific indices of results[i] to 1s
    results[i, sequences[[i]]] <- 1
  results
}

# Our vectorized training data
x_train <- vectorize_sequences(train_data)
# Our vectorized test data
x_test <- vectorize_sequences(test_data)
```

Here's what our samples look like now:

```{r}
str(x_train[1,])
```

We should also vectorize our labels, which is straightforward:

```{r}
# Our vectorized labels
y_train <- as.numeric(train_labels)
y_test <- as.numeric(test_labels)
```

## Validation Set

```{r}
val_indices <- 1:10000

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]

y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]
```

Now our data is ready to be fed into a neural network.

## Original Network

```{r}
library(keras)

model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

```{r}
model %>% compile(
  optimizer = optimizer_rmsprop(lr=0.001),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
) 
```


```{r}
history <- model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)
```

```{r}
plot(history)
```

The accuracy is plotted on the top panel and the loss on the bottom panel. 


Train with optimal no of epochs

```{r, echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

model %>% fit(x_train, y_train, epochs = 4, batch_size = 512)
results_test <- model %>% evaluate(x_test, y_test)
results_val <- model %>% evaluate(x_val, y_val)
```

```{r}
results_val
```
```{r}
results_test
```

## L2 Model

```{r}
model_l2 <- keras_model_sequential() %>% 
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(l=0.001),
              activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(l=0.001),
              activation = "relu") %>% 
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(l=0.001),
              activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
model_l2 %>% compile(
  optimizer = optimizer_rmsprop(lr=0.001),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```


l2 validation

```{r}
history_l2 <- model_l2 %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)
```
```{r}
plot(history_l2)
```

Train by changing no. of epochs
```{r}
model_l2 <- keras_model_sequential() %>% 
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(l=0.001),
              activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(l=0.001),
              activation = "relu") %>% 
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(l=0.001),
              activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
model_l2 %>% compile(
  optimizer = optimizer_rmsprop(lr=0.001),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```
```{r}
model_l2 %>% fit(x_train, y_train, epochs = 4, batch_size = 512)
results_l2_test <- model_l2 %>% evaluate(x_test, y_test)
results_l2_val <- model_l2 %>% evaluate(x_val, y_val)
```

```{r}
results_l2_val
```

```{r}
results_l2_test
```

##Dropout Model

```{r}
model_dropout <- keras_model_sequential() %>% 
  layer_dense(units = 32, activation = "relu", input_shape = c(10000)) %>% 
  layer_dropout(rate = 0.6) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(rate = 0.6) %>%
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
model_dropout %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)
```

Dropout validation

```{r}
history_do <- model_dropout %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

```

```{r}
plot(history_do)
```


Train model for 7 epochs

```{r}
model_dropout <- keras_model_sequential() %>% 
  layer_dense(units = 32, activation = "relu", input_shape = c(10000)) %>% 
  layer_dropout(rate = 0.6) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(rate = 0.6) %>%
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
model_dropout %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)
```

```{r}
model_dropout %>% fit(x_train, y_train, epochs = 7, batch_size = 512)
results_dropout_test <- model_dropout %>% evaluate(x_test, y_test)
results_dropout_val <- model_dropout %>% evaluate(x_val, y_val)
```


```{r}
results_dropout_val
```

```{r}
results_dropout_test
```

##Results Comparison

```{r}
v <- c(results_test$loss,results_l2_test$loss,results_dropout_test$loss,results_test$acc,results_l2_test$acc,results_dropout_test$acc,
       results_val$loss,results_l2_val$loss,results_dropout_val$loss,results_val$acc,results_l2_val$acc,results_dropout_val$acc)
v <- matrix(v,3,4)
rownames(v)<-c("Original","L2","Dropout")
colnames(v) <- c("Test Loss","Test Accuracy","Validation Loss","Validation Accuracy")
v
```


###Droupout Model gives better accuracy with less loss.